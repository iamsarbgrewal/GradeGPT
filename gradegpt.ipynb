{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install transformers\n! pip install accelerate -U\n! pip install evaluate\n! pip install -U nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling, TextDataset, EvalPrediction\nfrom scipy.special import softmax\nfrom sklearn.metrics import log_loss\nimport numpy as np\nimport torch\nimport re\nimport os\nimport nltk\nnltk.download('wordnet')\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"execution":{"iopub.status.busy":"2023-08-21T04:46:06.583588Z","iopub.execute_input":"2023-08-21T04:46:06.583969Z","iopub.status.idle":"2023-08-21T04:46:21.324795Z","shell.execute_reply.started":"2023-08-21T04:46:06.583931Z","shell.execute_reply":"2023-08-21T04:46:21.323712Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained('/kaggle/working/model')\ntokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/model')\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-08-21T05:03:36.006813Z","iopub.execute_input":"2023-08-21T05:03:36.007720Z","iopub.status.idle":"2023-08-21T05:03:42.386243Z","shell.execute_reply.started":"2023-08-21T05:03:36.007682Z","shell.execute_reply":"2023-08-21T05:03:42.385112Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Clear output folder\nimport os\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get datasets from Github","metadata":{}},{"cell_type":"code","source":"# !wget \"https://raw.githubusercontent.com/iamsarbgrewal/python-books/main/training.txt\"\n# !wget \"https://raw.githubusercontent.com/iamsarbgrewal/python-books/main/validation.txt\"\n# !wget \"https://raw.githubusercontent.com/iamsarbgrewal/python-books/main/test.txt\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text1 = \"A for loop is distinct from a while loop in Python when it is designed to iterate over a specific sequence or range of values. A for loop iterates over a sequence or range for a set number of times, allowing it to perform actions on each iteration. This approach simplifies tasks that require repetitive iteration, such as calculating a sum or filtering results based on a specified criterion. In contrast, a while loop repeatedly executes a block of code as long as a given condition remains true. When the condition becomes false, the loop terminates, and the sequence or range from which the loop was called is skipped. This distinction allows for more flexible and adaptable code that can handle a wide range of scenarios.\"\ntext2 = \"for loop iteration while loop repeat iterable true condition\"","metadata":{"execution":{"iopub.status.busy":"2023-08-21T05:02:34.237664Z","iopub.execute_input":"2023-08-21T05:02:34.238074Z","iopub.status.idle":"2023-08-21T05:02:34.243930Z","shell.execute_reply.started":"2023-08-21T05:02:34.238039Z","shell.execute_reply":"2023-08-21T05:02:34.242582Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model_output_path = '/kaggle/working/model'\ntrain_dataset = TextDataset(tokenizer=tokenizer, file_path=\"/kaggle/working/training.txt\", block_size=128)\nval_dataset = TextDataset(tokenizer=tokenizer, file_path=\"/kaggle/working/validation.txt\", block_size=128)\ntest_dataset = TextDataset(tokenizer=tokenizer, file_path=\"/kaggle/working/test.txt\", block_size=128)\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=model_output_path,\n    overwrite_output_dir=True,\n    num_train_epochs=4,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    save_strategy='steps',\n    save_steps=100,\n    evaluation_strategy='steps',\n    eval_steps=100,\n    logging_steps=100,\n    fp16=True,\n    save_total_limit=2,\n    report_to='none',\n    learning_rate=0.00001,\n    eval_accumulation_steps=1\n)\n\n# Create and train the model using the custom trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\ntrainer.train()\n# Save the model\ntrainer.save_model(model_output_path)\n# Save the tokenizer\ntokenizer.save_pretrained(model_output_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score(text1, text2):\n    # Tokenize and encode the texts\n    inputs = tokenizer([text1, text2], padding=True, truncation=True, return_tensors='pt')\n    vector1 = inputs.input_ids[0].reshape(1, -1)\n    vector2 =  inputs.input_ids[1].reshape(1, -1)\n    cosine_sim = cosine_similarity(vector1, vector2)[0][0]\n\n    # Calculate Jaccard similarity\n    def jaccard_similarity(text1, text2):\n        set1 = set(text1.split())\n        set2 = set(text2.split())\n        return len(set1 & set2) / len(set1 | set2)\n\n    jaccard_score = jaccard_similarity(text1, text2)\n    self_bleu_score = sentence_bleu([text2.split()], text1.split())\n\n    # Length-based similarity\n    length_similarity = 1 / (1 + abs(len(text1) - len(text2)))\n\n    # Combine scores using weighted average\n    weight_cosine = 0.6\n    weight_jaccard = 0.2\n    weight_self_bleu = 0.1\n    weight_length = 0.5\n\n    combined_score = (\n        weight_cosine * cosine_sim +\n        weight_jaccard * jaccard_score +\n        weight_length * length_similarity\n    )\n    return {\"Cosine score\" : cosine_sim, \"Jaccard score\" : jaccard_score, \"Length score\" : length_similarity, \"Combined Score\": combined_score}","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-08-21T05:03:48.323138Z","iopub.execute_input":"2023-08-21T05:03:48.323623Z","iopub.status.idle":"2023-08-21T05:03:48.335363Z","shell.execute_reply.started":"2023-08-21T05:03:48.323580Z","shell.execute_reply":"2023-08-21T05:03:48.334385Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"score(text1, text2)","metadata":{"execution":{"iopub.status.busy":"2023-08-21T05:04:11.550619Z","iopub.execute_input":"2023-08-21T05:04:11.551034Z","iopub.status.idle":"2023-08-21T05:04:11.565966Z","shell.execute_reply.started":"2023-08-21T05:04:11.550996Z","shell.execute_reply":"2023-08-21T05:04:11.564445Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'Cosine score': 0.4912103343990242,\n 'Jaccard score': 0.04819277108433735,\n 'Length score': 0.0014992503748125937,\n 'Combined Score': 0.3051143800436883}"},"metadata":{}}]},{"cell_type":"code","source":"def generate_response(prompt):    \n    model = GPT2LMHeadModel.from_pretrained('/kaggle/working/model')\n    tokenizer = GPT2Tokenizer.from_pretrained('/kaggle/working/model')\n    # Create the attention mask and pad token id\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n    attention_mask = torch.ones_like(input_ids)\n    \n    output = model.generate(\n        input_ids,\n        max_length=256,\n        num_beams=5,\n        attention_mask=attention_mask,\n        num_return_sequences=1, # Generate a single sequence\n        temperature=1,       # Controls randomness (higher for more diversity)\n        early_stopping=True,\n        top_k = 40\n    )\n\n    return tokenizer.decode(output[0], skip_special_tokens=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}